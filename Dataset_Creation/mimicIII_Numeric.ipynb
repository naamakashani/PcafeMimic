{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC-III Numeric Data Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "git clone to https://github.com/YerevaNN/mimic3-benchmarks\n",
    "follow the instructions in the README file to build benchmark data\n",
    "add to .\\mimic3-benchmarks\\mimic3models\\in_hospital_mortality\\logistic the following file:\n",
    "run the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "from mimic3benchmark.readers import InHospitalMortalityReader\n",
    "from mimic3models import common_utils\n",
    "from mimic3models.metrics import print_metrics_binary\n",
    "from mimic3models.in_hospital_mortality.utils import save_results\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import os\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "\n",
    "def read_and_extract_features(reader, period, features):\n",
    "    ret = common_utils.read_chunk(reader, reader.get_number_of_examples())\n",
    "    # ret = common_utils.read_chunk(reader, 100)\n",
    "    X = common_utils.extract_features_from_rawdata(ret['X'], ret['header'], period, features)\n",
    "    return (X, ret['y'], ret['name'])\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--C', type=float, default=1.0, help='inverse of L1 / L2 regularization')\n",
    "parser.add_argument('--l1', dest='l2', action='store_false')\n",
    "parser.add_argument('--l2', dest='l2', action='store_true')\n",
    "parser.set_defaults(l2=True)\n",
    "parser.add_argument('--period', type=str, default='all', help='specifies which period extract features from',\n",
    "                    choices=['first4days', 'first8days', 'last12hours', 'first25percent', 'first50percent', 'all'])\n",
    "parser.add_argument('--features', type=str, default='all', help='specifies what features to extract',\n",
    "                    choices=['all', 'len', 'all_but_len'])\n",
    "parser.add_argument('--data', type=str, help='Path to the data of in-hospital mortality task',\n",
    "                    default='directory with data of in-hospital mortality task')\n",
    "parser.add_argument('--output_dir', type=str, help='Directory relative which all output files are stored',\n",
    "                    default='.')\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "\n",
    "train_reader = InHospitalMortalityReader(dataset_dir=os.path.join(args.data, 'train'),\n",
    "                                         listfile=os.path.join(args.data, 'train_listfile.csv'),\n",
    "                                         period_length=48.0)\n",
    "\n",
    "val_reader = InHospitalMortalityReader(dataset_dir=os.path.join(args.data, 'train'),\n",
    "                                       listfile=os.path.join(args.data, 'val_listfile.csv'),\n",
    "                                       period_length=48.0)\n",
    "\n",
    "test_reader = InHospitalMortalityReader(dataset_dir=os.path.join(args.data, 'test'),\n",
    "                                        listfile=os.path.join(args.data, 'test_listfile.csv'),\n",
    "                                        period_length=48.0)\n",
    "\n",
    "print('Reading data and extracting features ...')\n",
    "(train_X, train_y, train_names) = read_and_extract_features(train_reader, args.period, args.features)\n",
    "(val_X, val_y, val_names) = read_and_extract_features(val_reader, args.period, args.features)\n",
    "(test_X, test_y, test_names) = read_and_extract_features(test_reader, args.period, args.features)\n",
    "#write train_X to csv file\n",
    "print('  train data shape = {}'.format(train_X.shape))\n",
    "print('  validation data shape = {}'.format(val_X.shape))\n",
    "print('  test data shape = {}'.format(test_X.shape))\n",
    "\n",
    "print('Imputing missing values ...')\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean', copy=True)\n",
    "imputer.fit(train_X)\n",
    "train_X = np.array(imputer.transform(train_X), dtype=np.float32)\n",
    "val_X = np.array(imputer.transform(val_X), dtype=np.float32)\n",
    "test_X = np.array(imputer.transform(test_X), dtype=np.float32)\n",
    "print('Normalizing the data to have zero mean and unit variance ...')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_X)\n",
    "train_X = scaler.transform(train_X)\n",
    "val_X = scaler.transform(val_X)\n",
    "test_X = scaler.transform(test_X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#create dir input/data_time_series\n",
    "os.makedirs('input/data_time_series', exist_ok=True)\n",
    "#change dir to input/data_time_series"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_X = pd.DataFrame(train_X)\n",
    "train_X.to_csv('train_X.csv', index=False)\n",
    "#write train_y to csv file\n",
    "train_y = pd.DataFrame(train_y)\n",
    "train_y.to_csv('train_y.csv', index=False)\n",
    "\n",
    "#write val_X to csv file\n",
    "val_X = pd.DataFrame(val_X)\n",
    "val_X.to_csv('val_X.csv', index=False)\n",
    "#write val_y to csv file\n",
    "val_y = pd.DataFrame(val_y)\n",
    "val_y.to_csv('val_y.csv', index=False)\n",
    "#write test_X to csv file\n",
    "test_X = pd.DataFrame(test_X)\n",
    "test_X.to_csv('test_X.csv', index=False)\n",
    "#write test_y to csv file\n",
    "test_y = pd.DataFrame(test_y)\n",
    "test_y.to_csv('test_y.csv', index=False)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# verify that u have the following files in the input/data_time_series directory"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
